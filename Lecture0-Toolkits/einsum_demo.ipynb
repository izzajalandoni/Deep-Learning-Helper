{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a50aefa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9396ea39",
   "metadata": {},
   "source": [
    "Einsum Tutorial\n",
    "\n",
    "Source: https://rockt.github.io/2018/04/30/einsum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daaf1bff",
   "metadata": {},
   "source": [
    "Matrix Transpose:\n",
    "$$B_{ji} = A_{ij}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97832773",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 3],\n",
       "        [1, 4],\n",
       "        [2, 5]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.arange(6).reshape(2, 3)\n",
    "torch.einsum('ij->ji', [a])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec7b096",
   "metadata": {},
   "source": [
    "Sum:\n",
    "\n",
    "$$b = \\sum_{i}sum_{j}A_{ij} = A_{ij}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21702d94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(15)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(6).reshape(2, 3)\n",
    "torch.einsum('ij->', [a])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4186ee",
   "metadata": {},
   "source": [
    "Column Sum:\n",
    "\n",
    "$$b_{j} = \\sum_{i}A_{ij} = A_{ij}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29703620",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 5, 7])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(6).reshape(2, 3)\n",
    "torch.einsum('ij->j', [a])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55814955",
   "metadata": {},
   "source": [
    "Row Sum:\n",
    "\n",
    "$$b_{i} = \\sum_{j}A_{ij} = A_{ij}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d99fbf36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3, 12])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(6).reshape(2, 3)\n",
    "torch.einsum('ij->i', [a])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75158b88",
   "metadata": {},
   "source": [
    "Matrix-Vector Multiplication:\n",
    "\n",
    "$$C_{ij} = \\sum_{k}A_{ik}b_{k} = A_{ik}B_{kj}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78ac2532",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5, 14])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(6).reshape(2, 3)\n",
    "b = torch.arange(3)\n",
    "torch.einsum('ik,k->i', [a, b])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de925cd3",
   "metadata": {},
   "source": [
    "Matrix-Matrix Multiplication:\n",
    "\n",
    "$$C_{ij} = \\sum_{k}A_{ik}b_{kj} = A_{ik}b_{kj}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76814a19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 25,  28,  31,  34,  37],\n",
       "        [ 70,  82,  94, 106, 118]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(6).reshape(2, 3)\n",
    "b = torch.arange(15).reshape(3, 5)\n",
    "torch.einsum('ik,kj->ij', [a, b])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3d3802",
   "metadata": {},
   "source": [
    "Dot Product Vector:\n",
    "\n",
    "$$c_{i} = \\sum_{i}a_{i}b_{i} = a_{i}b_{i}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3bd16837",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(14)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(3)\n",
    "b = torch.arange(3,6)  # -- a vector of length 3 containing [3, 4, 5]\n",
    "torch.einsum('i,i->', [a, b])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117d9098",
   "metadata": {},
   "source": [
    "Dot Product Matrix:\n",
    "\n",
    "$$c_{i} = \\sum_{i}\\sum_{j}A_{ij}B_{ij} = A_{ij}B_{ij}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0de8eca0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(145)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(6).reshape(2, 3)\n",
    "b = torch.arange(6,12).reshape(2, 3)\n",
    "torch.einsum('ij,ij->', [a, b])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d95c50",
   "metadata": {},
   "source": [
    "Hadamard Product:\n",
    "\n",
    "$$C_{ij} = A_{ij}B_{ij}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1cf9021c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  7, 16],\n",
       "        [27, 40, 55]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(6).reshape(2, 3)\n",
    "b = torch.arange(6,12).reshape(2, 3)\n",
    "torch.einsum('ij,ij->ij', [a, b])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1445c8",
   "metadata": {},
   "source": [
    "Bilinear Transformation:\n",
    "\n",
    "$$D_{ij} = \\sum_{k}\\sum_{l}A_{ik}B_{jkl}C_{il} = A_{ik}B_{jkl}C_{il}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5839144",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.3644, -1.4066, -2.8853, -3.3441,  2.4853],\n",
       "        [-0.7584,  2.4459,  1.3680,  5.7902, -3.5133]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(2,3)\n",
    "b = torch.randn(5,3,7)\n",
    "c = torch.randn(2,7)\n",
    "torch.einsum('ik,jkl,il->ij', [a, b, c])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e5a46b",
   "metadata": {},
   "source": [
    "Attention:\n",
    "\n",
    "Rocktäschel, Grefenstette, Hermann, Kocisky and Blunsom. Reasoning about Entailment with Neural Attention. in: International Conference on Learning Representations (ICLR). 2016\n",
    "\n",
    "$$M_{t} = tanh(W^{y}Y + (W^{h}h_{t} + W^{r}r_{t-1}) \\bigotimes e_{L}) \\enspace\\enspace M_{t} ϵ \\mathbb{R}^{kXL} $$\n",
    "$$\\alpha_{t} = softmax(w^{T}M_{t}) \\enspace\\enspace\\enspace\\enspace\\enspace\\enspace\\enspace\\enspace\\enspace\\enspace\\enspace\\enspace\\enspace\\enspace\\enspace\\enspace\\enspace\\enspace\\enspace\\enspace \\alpha_{t} ϵ \\mathbb{R}^{L} $$\n",
    "$$r_{t} = Y\\alpha^{T}_{t} + tanh(W^{t}r_{t-1}) \\enspace\\enspace\\enspace\\enspace\\enspace\\enspace\\enspace\\enspace\\enspace\\enspace\\enspace\\enspace\\enspace\\enspace\\enspace\\enspace\\enspace\\enspace r_{t} ϵ \\mathbb{R}^{k} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7728959e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2531, 0.0308, 0.4751, 0.2212, 0.0198],\n",
       "        [0.1354, 0.2035, 0.4196, 0.1273, 0.1142],\n",
       "        [0.0536, 0.5476, 0.0097, 0.0278, 0.3613]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def random_tensors(shape, num=1, requires_grad=False):\n",
    "  tensors = [torch.randn(shape, requires_grad=requires_grad) for i in range(0, num)]\n",
    "  return tensors[0] if num == 1 else tensors\n",
    "\n",
    "# Parameters\n",
    "# -- [hidden_dimension]\n",
    "bM, br, w = random_tensors([7], num=3, requires_grad=True)\n",
    "# -- [hidden_dimension x hidden_dimension]\n",
    "WY, Wh, Wr, Wt = random_tensors([7, 7], num=4, requires_grad=True)\n",
    "\n",
    "# Single application of attention mechanism \n",
    "def attention(Y, ht, rt1):\n",
    "  # -- [batch_size x hidden_dimension] \n",
    "  tmp = torch.einsum(\"ik,kl->il\", [ht, Wh]) + torch.einsum(\"ik,kl->il\", [rt1, Wr])\n",
    "  Mt = F.tanh(torch.einsum(\"ijk,kl->ijl\", [Y, WY]) + tmp.unsqueeze(1).expand_as(Y) + bM)\n",
    "  # -- [batch_size x sequence_length]\n",
    "  at = F.softmax(torch.einsum(\"ijk,k->ij\", [Mt, w])) \n",
    "  # -- [batch_size x hidden_dimension]\n",
    "  rt = torch.einsum(\"ijk,ij->ik\", [Y, at]) + F.tanh(torch.einsum(\"ij,jk->ik\", [rt1, Wt]) + br)\n",
    "  # -- [batch_size x hidden_dimension], [batch_size x sequence_dimension]\n",
    "  return rt, at\n",
    "\n",
    "# Sampled dummy inputs\n",
    "# -- [batch_size x sequence_length x hidden_dimension]\n",
    "Y = random_tensors([3, 5, 7])\n",
    "# -- [batch_size x hidden_dimension]\n",
    "ht, rt1 = random_tensors([3, 7], num=2)\n",
    "\n",
    "rt, at = attention(Y, ht, rt1)\n",
    "at  # -- print attention weights"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_course",
   "language": "python",
   "name": "dl_course"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
