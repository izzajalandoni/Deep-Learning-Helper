{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7e3dab4",
   "metadata": {},
   "source": [
    "Source: https://pytorch.org/docs/stable/generated/torch.nn.KLDivLoss.html\n",
    "\n",
    "Comparison between CrossEntropy and KL-Divergence:\n",
    "https://www.tertiaryinfotech.com/comparing-cross-entropy-and-kl-divergence-loss/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da4bd0a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\envs\\torch110py3\\lib\\site-packages\\torch\\nn\\functional.py:2747: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "a = torch.log_softmax(torch.tensor([[0.8000, 0.1500, 0.0500]]), dim=1)\n",
    "b = torch.softmax(torch.tensor([[0.8000, 0.1500, 0.0500]]), dim=1)\n",
    "\n",
    "criterion = nn.KLDivLoss()\n",
    "loss = criterion(a, b)\n",
    "\n",
    "assert loss == 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5301ef4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch110py3] *",
   "language": "python",
   "name": "conda-env-torch110py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
